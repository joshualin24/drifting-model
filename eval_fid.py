import torch
import os
import argparse
import shutil
from pathlib import Path
from tqdm import tqdm
from torchvision import transforms
from pytorch_fid import fid_score

# è¼‰å…¥ä½ è¨“ç·´è…³æœ¬ä¸­çš„æ¨¡å‹å®šç¾©èˆ‡é¡åˆ¥
from model import DriftDiT_models
from utils import load_checkpoint
from train_galaxy import Galaxy10Dataset  # ç¢ºä¿ train.py åœ¨åŒä¸€å€‹ç›®éŒ„

def run_evaluation(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f" usando device: {device}")

    # --- 1. æº–å‚™è·¯å¾‘ ---
    output_path = Path(args.eval_dir)
    real_path = output_path / "real_samples"
    fake_path = output_path / "fake_samples"
    
    for p in [real_path, fake_path]:
        if p.exists(): shutil.rmtree(p)
        p.mkdir(parents=True)

    # --- 2. æº–å‚™çœŸå¯¦æ•¸æ“šé›† (Reference) ---
    print("ğŸ“¦ Extracting real samples from Galaxy10...")
    transform_real = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(args.img_size),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    
    # é€™è£¡è¼‰å…¥æ¸¬è©¦é›†æˆ–é©—è­‰é›†ä¾†ä½œç‚ºå°æ¯”åŸºæº–
    dataset = Galaxy10Dataset(root="./data", train=False, transform=transform_real)
    num_eval = min(len(dataset), args.num_samples)
    
    for i in tqdm(range(num_eval), desc="Saving Real Images"):
        img_tensor, _ = dataset[i]
        # åæ¨™æº–åŒ–
        img = transforms.ToPILImage()(img_tensor * 0.5 + 0.5)
        img.save(real_path / f"real_{i}.png")

    # --- 3. è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹ ---
    print(f"ğŸš€ Loading model from {args.checkpoint}...")
    model_fn = DriftDiT_models[args.model_type]
    model = model_fn(
        img_size=args.img_size,
        in_channels=3,
        num_classes=10
    ).to(device)

    # è¼‰å…¥ checkpoint (ä¸»è¦æå– EMA æ¬Šé‡)
    ckpt = torch.load(args.checkpoint, map_location=device)
    if "ema" in ckpt:
        model.load_state_dict(ckpt["ema"])
        print("âœ… EMA weights loaded successfully.")
    else:
        model.load_state_dict(ckpt["model"])
        print("âš ï¸ EMA not found, using raw model weights.")
    
    model.eval()

    # --- 4. ç”Ÿæˆæ¨£æœ¬ ---
    print(f"ğŸ¨ Generating {args.num_samples} fake samples...")
    batch_size = args.batch_size
    generated_count = 0
    
    pbar = tqdm(total=args.num_samples, desc="Generating Images")
    while generated_count < args.num_samples:
        curr_batch = min(batch_size, args.num_samples - generated_count)
        
        # éš¨æ©Ÿæˆ–å‡å‹»æŠ½å–æ˜Ÿç³»é¡åˆ¥ (0-9)
        labels = torch.randint(0, 10, (curr_batch,), device=device)
        noise = torch.randn(curr_batch, 3, args.img_size, args.img_size, device=device)
        
        with torch.no_grad():
            # ä½¿ç”¨ CFG Scale (æ¨è–¦ 1.5 - 2.0 æå‡å“è³ª)
            samples = model.forward_with_cfg(noise, labels, alpha=args.cfg_scale)
            samples = (samples * 0.5 + 0.5).clamp(0, 1)
        
        for j in range(samples.size(0)):
            img = transforms.ToPILImage()(samples[j].cpu())
            img.save(fake_path / f"gen_{generated_count}.png")
            generated_count += 1
        
        pbar.update(curr_batch)
    pbar.close()

    # --- 5. è¨ˆç®— FID ---
    print(f"ğŸ“Š Calculating FID between {real_path} and {fake_path}...")
    fid_value = fid_score.calculate_fid_given_paths(
        paths=[str(real_path), str(fake_path)],
        batch_size=args.batch_size,
        device=device,
        dims=2048
    )

    print("\n" + "="*40)
    print(f"ğŸ† FINAL FID SCORE: {fid_value:.4f}")
    print("="*40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to ckpt_epX.pt")
    parser.add_argument("--model_type", type=str, default="DriftDiT-Small")
    parser.add_argument("--img_size", type=int, default=32)
    parser.add_argument("--num_samples", type=int, default=2048, help="FID is more accurate with > 2048 images")
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--cfg_scale", type=float, default=1.5)
    parser.add_argument("--eval_dir", type=str, default="./fid_evaluation")
    
    args = parser.parse_args()
    run_evaluation(args)